{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tvAKGat01Sd"
      },
      "source": [
        "# **Penting**\n",
        "- Jangan mengubah atau menambahkan cell text yang sudah disediakan, Anda hanya perlu mengerjakan cell code yang sudah disediakan.\n",
        "- Pastikan seluruh kriteria memiliki output yang sesuai, karena jika tidak ada output dianggap tidak selesai.\n",
        "- Misal, Anda menggunakan df = df.dropna() silakan gunakan df.isnull().sum() sebagai tanda sudah berhasil. Silakan sesuaikan seluruh output dengan perintah yang sudah disediakan.\n",
        "- Pastikan Anda melakukan Run All sebelum mengirimkan submission untuk memastikan seluruh cell berjalan dengan baik.\n",
        "- Pastikan Anda menggunakan variabel df dari awal sampai akhir dan tidak diperbolehkan mengganti nama variabel tersebut.\n",
        "- Hapus simbol pagar (#) pada kode yang bertipe komentar jika Anda menerapkan kriteria tambahan\n",
        "- Biarkan simbol pagar (#) jika Anda tidak menerapkan kriteria tambahan\n",
        "- Pastikan Anda mengerjakan sesuai section yang sudah diberikan tanpa mengubah judul atau header yang disediakan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKADPWcFKlj3"
      },
      "source": [
        "# **1. Import Library**\n",
        "Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "BlmvjLY9M4Yj"
      },
      "outputs": [],
      "source": [
        "#Type your code here\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3YIEnAFKrKL"
      },
      "source": [
        "# **2. Memuat Dataset dari Hasil Clustering**\n",
        "Memuat dataset hasil clustering dari file CSV ke dalam variabel DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GHCGNTyrM5fS"
      },
      "outputs": [],
      "source": [
        "# Gunakan dataset hasil clustering yang memiliki fitur Target\n",
        "# Silakan gunakan dataset data_clustering jika tidak menerapkan Interpretasi Hasil Clustering [Advanced]\n",
        "# Silakan gunakan dataset data_clustering_inverse jika menerapkan Interpretasi Hasil Clustering [Advanced]\n",
        "# Lengkapi kode berikut\n",
        "df = pd.read_csv('/Users/yudisdwi/Documents/BMLP/data_clustering_inverse.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bCsep0NZ0LUf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionAmount</th>\n",
              "      <th>CustomerAge</th>\n",
              "      <th>TransactionDuration</th>\n",
              "      <th>LoginAttempts</th>\n",
              "      <th>AccountBalance</th>\n",
              "      <th>TransactionType</th>\n",
              "      <th>Location</th>\n",
              "      <th>Channel</th>\n",
              "      <th>CustomerOccupation</th>\n",
              "      <th>TransactionAmount_bin</th>\n",
              "      <th>AccountBalance_bin</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.09</td>\n",
              "      <td>70.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5112.21</td>\n",
              "      <td>Debit</td>\n",
              "      <td>San Diego</td>\n",
              "      <td>ATM</td>\n",
              "      <td>Doctor</td>\n",
              "      <td>Low</td>\n",
              "      <td>Medium</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>376.24</td>\n",
              "      <td>68.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>13758.91</td>\n",
              "      <td>Debit</td>\n",
              "      <td>Houston</td>\n",
              "      <td>ATM</td>\n",
              "      <td>Doctor</td>\n",
              "      <td>Medium</td>\n",
              "      <td>High</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>126.29</td>\n",
              "      <td>19.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1122.35</td>\n",
              "      <td>Debit</td>\n",
              "      <td>Mesa</td>\n",
              "      <td>Online</td>\n",
              "      <td>Student</td>\n",
              "      <td>Low</td>\n",
              "      <td>Low</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>184.50</td>\n",
              "      <td>26.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8569.06</td>\n",
              "      <td>Debit</td>\n",
              "      <td>Raleigh</td>\n",
              "      <td>Online</td>\n",
              "      <td>Student</td>\n",
              "      <td>Low</td>\n",
              "      <td>Medium</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>92.15</td>\n",
              "      <td>18.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>781.68</td>\n",
              "      <td>Debit</td>\n",
              "      <td>Oklahoma City</td>\n",
              "      <td>ATM</td>\n",
              "      <td>Student</td>\n",
              "      <td>Low</td>\n",
              "      <td>Low</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionAmount  CustomerAge  TransactionDuration  LoginAttempts  \\\n",
              "0              14.09         70.0                 81.0            1.0   \n",
              "1             376.24         68.0                141.0            1.0   \n",
              "2             126.29         19.0                 56.0            1.0   \n",
              "3             184.50         26.0                 25.0            1.0   \n",
              "4              92.15         18.0                172.0            1.0   \n",
              "\n",
              "   AccountBalance TransactionType       Location Channel CustomerOccupation  \\\n",
              "0         5112.21           Debit      San Diego     ATM             Doctor   \n",
              "1        13758.91           Debit        Houston     ATM             Doctor   \n",
              "2         1122.35           Debit           Mesa  Online            Student   \n",
              "3         8569.06           Debit        Raleigh  Online            Student   \n",
              "4          781.68           Debit  Oklahoma City     ATM            Student   \n",
              "\n",
              "  TransactionAmount_bin AccountBalance_bin  Target  \n",
              "0                   Low             Medium       1  \n",
              "1                Medium               High       0  \n",
              "2                   Low                Low       0  \n",
              "3                   Low             Medium       1  \n",
              "4                   Low                Low       0  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tampilkan 5 baris pertama dengan function head.\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkPem5eWL2UP"
      },
      "source": [
        "# **3. Data Splitting**\n",
        "Tahap Data Splitting bertujuan untuk memisahkan dataset menjadi dua bagian: data latih (training set) dan data uji (test set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2182 entries, 0 to 2181\n",
            "Data columns (total 12 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   TransactionAmount      2182 non-null   float64\n",
            " 1   CustomerAge            2182 non-null   float64\n",
            " 2   TransactionDuration    2182 non-null   float64\n",
            " 3   LoginAttempts          2182 non-null   float64\n",
            " 4   AccountBalance         2182 non-null   float64\n",
            " 5   TransactionType        2152 non-null   object \n",
            " 6   Location               2154 non-null   object \n",
            " 7   Channel                2159 non-null   object \n",
            " 8   CustomerOccupation     2162 non-null   object \n",
            " 9   TransactionAmount_bin  2182 non-null   object \n",
            " 10  AccountBalance_bin     2182 non-null   object \n",
            " 11  Target                 2182 non-null   int64  \n",
            "dtypes: float64(5), int64(1), object(6)\n",
            "memory usage: 204.7+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OubAW-7ONKVj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ukuran data latih: (1745, 11)\n",
            "Ukuran data uji: (437, 11)\n"
          ]
        }
      ],
      "source": [
        "# Menggunakan train_test_split() untuk melakukan pembagian dataset.\n",
        "# Pisahkan fitur dan target\n",
        "X = df.drop('Target', axis=1)\n",
        "y = df['Target']\n",
        "\n",
        "# Identifikasi kolom kategori & numerik\n",
        "cat_cols = X.select_dtypes(include=['object']).columns\n",
        "num_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Label encoding untuk kolom kategori\n",
        "encoders = {}\n",
        "for col in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col].astype(str))  # handle NaN\n",
        "    encoders[col] = le\n",
        "\n",
        "# StandardScaler untuk kolom numerik\n",
        "scaler = StandardScaler()\n",
        "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "# Split data ke train dan test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Cek hasil\n",
        "print(\"Ukuran data latih:\", X_train.shape)\n",
        "print(\"Ukuran data uji:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVPbB03CMhTT"
      },
      "source": [
        "# **4. Membangun Model Klasifikasi**\n",
        "Setelah memilih algoritma klasifikasi yang sesuai, langkah selanjutnya adalah melatih model menggunakan data latih.\n",
        "\n",
        "Berikut adalah rekomendasi tahapannya.\n",
        "1. Menggunakan algoritma klasifikasi yaitu Decision Tree.\n",
        "2. Latih model menggunakan data yang sudah dipisah."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JYxBe87NLDk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                Model  Accuracy  Precision  Recall  F1-Score\n",
            "0  Decision Tree (DT)       1.0        1.0     1.0       1.0\n",
            "\n",
            "Confusion Matrix:\n",
            " [[156   0   0]\n",
            " [  0 134   0]\n",
            " [  0   0 147]]\n"
          ]
        }
      ],
      "source": [
        "# Buatlah model klasifikasi menggunakan Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42).fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Fungsi evaluasi multiclass\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    results = {\n",
        "        'Confusion Matrix': cm,\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision (macro)': precision_score(y_test, y_pred, average='macro'),\n",
        "        'Recall (macro)': recall_score(y_test, y_pred, average='macro'),\n",
        "        'F1-Score (macro)': f1_score(y_test, y_pred, average='macro')\n",
        "    }\n",
        "    return results\n",
        "\n",
        "# Evaluasi model Decision Tree\n",
        "dt_results = evaluate_model(dt, X_test, y_test)\n",
        "\n",
        "# Buat ringkasan hasil \n",
        "summary_df = pd.DataFrame([{\n",
        "    'Model': 'Decision Tree (DT)',\n",
        "    'Accuracy': dt_results['Accuracy'],\n",
        "    'Precision': dt_results['Precision (macro)'],\n",
        "    'Recall': dt_results['Recall (macro)'],\n",
        "    'F1-Score': dt_results['F1-Score (macro)']\n",
        "}])\n",
        "\n",
        "# Tampilkan hasil\n",
        "print(summary_df)\n",
        "print(\"\\nConfusion Matrix:\\n\", dt_results['Confusion Matrix'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "P_AakAxghYv-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['decision_tree_model.h5']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Menyimpan Model\n",
        "import joblib\n",
        "joblib.dump(dt, 'decision_tree_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epO4HhrzBXMg"
      },
      "source": [
        "# **5. Memenuhi Kriteria Skilled dan Advanced dalam Membangun Model Klasifikasi**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNOEZk24uiXu"
      },
      "source": [
        "**Biarkan kosong jika tidak menerapkan kriteria skilled atau advanced**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB_8LIWMATl6"
      },
      "outputs": [],
      "source": [
        "# Melatih model menggunakan algoritma klasifikasi scikit-learn selain Decision Tree.\n",
        "rf = RandomForestClassifier().fit(X_train, y_train)\n",
        "rf_results = evaluate_model(rf, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "bRlKm5BVAT91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      Model  Accuracy  Precision  Recall  F1-Score\n",
            "0  Random Forest Classifier       1.0        1.0     1.0       1.0\n",
            "\n",
            "Confusion Matrix:\n",
            " [[156   0   0]\n",
            " [  0 134   0]\n",
            " [  0   0 147]]\n"
          ]
        }
      ],
      "source": [
        "# Menampilkan hasil evaluasi akurasi, presisi, recall, dan F1-Score pada seluruh algoritma yang sudah dibuat.\n",
        "\n",
        "# Buat ringkasan hasil \n",
        "summary_df = pd.DataFrame([{\n",
        "    'Model': 'Random Forest Classifier',\n",
        "    'Accuracy': rf_results['Accuracy'],\n",
        "    'Precision': rf_results['Precision (macro)'],\n",
        "    'Recall': rf_results['Recall (macro)'],\n",
        "    'F1-Score': rf_results['F1-Score (macro)']\n",
        "}])\n",
        "\n",
        "# Tampilkan hasil\n",
        "print(summary_df)\n",
        "print(\"\\nConfusion Matrix:\\n\", dt_results['Confusion Matrix'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "dUPItkbXBNkO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['explore_rf_classification.h5']"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Menyimpan Model Selain Decision Tree\n",
        "# Model ini bisa lebih dari satu\n",
        "import joblib\n",
        "joblib.dump(rf, 'explore_rf_classification.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u23H2guj-h9h"
      },
      "source": [
        "Hyperparameter Tuning Model\n",
        "\n",
        "Pilih salah satu algoritma yang ingin Anda tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "dFCTxJJq-m-l"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial accuracy on test set (without tuning): 1.00\n",
            "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=2, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=gini, max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=5, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=10, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=5, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=10, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=5, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=gini, max_depth=30, min_samples_split=10, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=2, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=5, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=20, min_samples_split=10, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=5, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=2, n_estimators=300; total time=   0.5s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=10, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=5, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=10, n_estimators=300; total time=   0.4s\n",
            "[CV] END criterion=entropy, max_depth=30, min_samples_split=10, n_estimators=300; total time=   0.4s\n",
            "Best parameters (Grid Search): {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Accuracy after Grid Search: 1.00\n"
          ]
        }
      ],
      "source": [
        "# Lakukan Hyperparameter Tuning dan Latih ulang.\n",
        "# Lakukan dalam satu cell ini saja.\n",
        " \n",
        " \n",
        "# Evaluasi awal model tanpa tuning\n",
        "initial_score = rf.score(X_test, y_test)\n",
        "print(f\"Initial accuracy on test set (without tuning): {initial_score:.2f}\")\n",
        "\n",
        "# Definisikan parameter grid untuk Grid Search\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        " \n",
        "# Inisialisasi GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        " \n",
        "# Output hasil terbaik\n",
        "print(f\"Best parameters (Grid Search): {grid_search.best_params_}\")\n",
        "best_rf_grid = grid_search.best_estimator_\n",
        " \n",
        "# Evaluasi performa model pada test set\n",
        "grid_search_score = best_rf_grid.score(X_test, y_test)\n",
        "print(f\"Accuracy after Grid Search: {grid_search_score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "1g6EPSSWxjcQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                              Model  Accuracy  Precision  Recall  F1-Score\n",
            "0  Random Forest (Best Grid Search)       1.0        1.0     1.0       1.0\n",
            "\n",
            "Confusion Matrix:\n",
            " [[156   0   0]\n",
            " [  0 134   0]\n",
            " [  0   0 147]]\n"
          ]
        }
      ],
      "source": [
        "# Menampilkan hasil evaluasi akurasi, presisi, recall, dan F1-Score pada algoritma yang sudah dituning.\n",
        "best_rf_results = evaluate_model(best_rf_grid, X_test, y_test)\n",
        "\n",
        "# Buat ringkasan hasil ke dalam DataFrame\n",
        "summary_df = pd.DataFrame([{\n",
        "    'Model': 'Random Forest (Best Grid Search)',\n",
        "    'Accuracy': best_rf_results['Accuracy'],\n",
        "    'Precision': best_rf_results['Precision (macro)'],\n",
        "    'Recall': best_rf_results['Recall (macro)'],\n",
        "    'F1-Score': best_rf_results['F1-Score (macro)']\n",
        "}])\n",
        "\n",
        "# Tampilkan hasil evaluasi\n",
        "print(summary_df)\n",
        "print(\"\\nConfusion Matrix:\\n\", best_rf_results['Confusion Matrix'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "7UJNcVP--n7S"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tuning_classification.h5']"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Menyimpan Model hasil tuning\n",
        "import joblib\n",
        "joblib.dump(best_rf_grid, 'tuning_classification.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hs4Xp4OiGEk"
      },
      "source": [
        "End of Code"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bmlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
